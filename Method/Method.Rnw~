<<<extern-method,eval=TRUE,echo=FALSE,warning=FALSE>>=
read_chunk('./latin_R/plot.R')
@ 

Here we wish to put forth some new statistical methodology towards solving the problem of cell-type deconvolution. The primary focus of the development will involve probe-level microarray data. However the methods presented here are very applicable to gene-level microarray data and potentially to other technologies such as RNA-seq. 

\section{A Basic Example}

First we will start with a basic deconvolution example to give the reader an understanding of our methodology in a simple setting. Assume that we have three cell samples $A,B$ and $C$. Furthermore assume that each these samples is a mixture of two cell types. Let $A$ be a sample consisting only of cell of type one, $B$ a sample of only cell type two and $C$ be a mixture of the two cell types. We would like to deconvolve sample $C$ and determine how the cell types are mixed. 

Generally, the way a microarray experiment is conducted is that the scientist extracts oligos from the cell samples and dilutes them into an aqueous solution called a ``hybridization solution.'' Simply, this hybridiziation solution is some water-like mixture with the oligos from the sample floating around. Given a hybridization solution, assume that we have microarray technology to measure the expression of $N$ oligos in the solution. Let $\eta_{An}$ be the concentration of oligo $n$ in the hybridization solution made from sample $A$. Similarly define $\eta_{Bn}$ and $\eta_{Cn}$. Now since sample $C$ is a mixture of cell types $A$ and $B$ then then its hybridization solution is going to effectively be a mixture of the hybridization solutions of cell types $A$ and $B$. Thus the concentration of oligo $n$ in the hybridization solution created from sample $C$ is going to be a convex combination of the concentration of oligo $n$ in samples $A$ and $B$. That is, 
\[
\eta_{Cn} = p_A\eta_{An} + p_B\eta_{Bn}
\]
where $p_A$ and $p_B$ are the proportions the sample $A$ and $B$ hybridization solutions are mixed together to make the sample $C$ hybridization solution. So $p_A,p_B\geq 0$ and $p_A+p_B = 1$. 

What we want is a model relating the oligo expressions measured by the microarray to the amount of oligos in the experiment. We assume that the expressions of oligos measured on the microarray are a function of the concentration, or amount, of oligos in solution. To this end we assume a linear model (on a $\log$ versus $\log$ level) for the relationship between the concentration of oligos in the hybridization solution and the expression value measured by the microarray. Thus, for example, if $I_{An}$ is the probe intensity measurement of the $n^{th}$ oligo given by the microarray experiment on sample $A$ then we assume that 
\[
Y_{An}\overset{def}{=}\log_2\left(I_{An}\right) = \theta_n + \gamma\log_2\left(\eta_{An}\right) + \epsilon_{An}.
\]
We assume the same model for the microarray measurements from samples $B$ and $C$ such that 
\[
Y_{Bn}\overset{def}{=}\log_2\left(I_{Bn}\right) = \theta_n + \gamma\log_2\left(\eta_{Bn}\right) + \epsilon_{Bn}
\]
and 
\[
Y_{Cn}\overset{def}{=}\log_2\left(I_{Cn}\right) = \theta_n + \gamma\log_2\left(\eta_{Cn}\right) + \epsilon_{Cn}
\]
where the quantities are defined similarly. There are couple notes to this model. Firstly the intercept term $\theta_n$ is unique to each of the $n$ oligos but is shared among experiments. Secondly the slope of our linear model, $\gamma$, is universal and depends neither on the sample $A,B$ or $C$ nor the oligo $n$. Finally there are error terms $\epsilon_{An},\epsilon_{Bn}$ and $\epsilon_{Cn}$ which we presume are random with mean zero and some finite shared variance $\sigma^2$. Furthermore we assume these epsilon terms are mutually independent not only across samples $A,B$ and $C$ but also across the oligos $n=1,\ldots,N$. Note that we use $\log$ base two as is tradition in DNA microarray analysis (c.f. \citeauthor{Irizarry2003} \citeyear{Irizarry2003}). Now that we have rigorously defined the quantities of interest and posited a model between the expressions and oligos we are ready to put forth a deconvolution estimating procedure. 

In order to estimate $p_A$ and $p_B$ we need one last piece of information. We need to assume that there is a ``marker oligo'' for each cell type. For a given cell type we define a marker oligo as an oligo that is expressed in that cell type and not in any other. Thus these marker oligos are indicative of one particular cell type and not any other. In particular we assume there is an oligo $n_1$ that is expressed by cells of type one only and not of cells of type two. Similarly we assume there is some marker oligo $n_2$ that is expressed by cells of type two and not type one. In terms of notation this means that we are assuming some marker oligos $n_1$ and $n_2$ such that $\eta_{An_2}=0$ and $\eta_{Bn_1}=0$. This assumption will be crucial to our deconvolution pursuit because it means that 
\[
\eta_{Cn_1} = p_A\eta_{An_1} + p_B\eta_{Bn_1} = p_A\eta_{An_1} 
\]
and similarly 
\[
\eta_{Cn_2} = p_A\eta_{An_2} + p_B\eta_{Bn_2} = p_B\eta_{Bn_2}. 
\]
This fact will ultimately allow us to get estimates of $p_A$ and $p_B$ from the microarray expressions by exploiting our linear model. For the type one marker oligo $n_1$ we have an microarray expression measurement $Y_{An_1}$ from sample $A$ and measurement $Y_{Cn_1}$ from sample $C$. We modeled these as 
\[
Y_{An_1} = \theta_{n_1} + \gamma\log_2\left(\eta_{An_1}\right) + \epsilon_{An_1}.
\]
and
\[
Y_{Cn_1} = \theta_{n_1} + \gamma\log_2\left(\eta_{Cn_1}\right) + \epsilon_{Cn_1}.
\]
However we know that $\eta_{Cn_1} = p_A\eta_{An_1}$ hence 
\[
\begin{aligned}
Y_{Cn_1} &= \theta_{n_1} + \gamma\log_2\left(p_A\eta_{An_1}\right) + \epsilon_{Cn_1}\\
&= \theta_{n_1} + \gamma\log_2\left(p_A\right)+\gamma\log_2\left(\eta_{An_1}\right) + \epsilon_{Cn_1}\\
\end{aligned}
\]
This means that 
\[
Y_{Cn_1}-Y_{An_1} = \gamma\log_2\left(p_A\right) + \epsilon_{Cn_1} - \epsilon_{An_1}
\]
hence
\[
\exp_2\left(\frac{Y_{Cn_1}-Y_{An_1}}{\gamma}\right) = \lambda_{n_1} p_A \text{ where } \lambda_{n_1}\overset{def}{=}\exp_2\left(\frac{\epsilon_{Cn_1} - \epsilon_{An_1}}{\gamma}\right).
\]
The term $\lambda_{n_1}$ is a term that depends on the random errors which ends up as a multiplicative error term once we exponentiate. Thus if we can find some estiamtor $\widehat{\gamma}$ of $\gamma$ we can form the estimator
\[
\widehat{q_A} = \exp_2\left(\frac{Y_{Cn_1}-Y_{An_1}}{\widehat{\gamma}}\right)
\]
with the hopes that if $\widehat{\gamma}\rightarrow\gamma$ then $\widehat{q_A}\rightarrow \lambda_{n_1} p_A$. In a similar fashion we can define $\widehat{q_B} = \exp_2\left(\frac{Y_{Cn_2}-Y_{Bn_2}}{\widehat{\gamma}}\right)$. Importantly, however, we cannot guarantee that $\widehat{q_A},\widehat{q_B}\leq 1$ nor that $\widehat{q_A}+\widehat{q_B} = 1$. Firstly, even if our data exactly follows the linear model we assumed we can't guarantee that the lambda terms $\lambda_{n_1}$ and $\lambda_{n_2}$ don't perturb the estimates to be bigger than one. Furthermore, the two lambda terms are assumed independent so they can move $p_A$ and $p_B$ independently in ways such that they do not sum to one. Secondly, as a matter of practicality, our data will not exactly follow our linear model and so these estimates do not need to be as well behaved in practice as in theory. At most we can guarantee that $\widehat{q_A}$ and $\widehat{q_B}$ will be positive since they are both exponential terms. Ensuring that our estimators of $p_A$ and $p_B$ satisfy the sum-to-one and non-negativity constraints of a proportion estimator is important so we re-normalize $\widehat{q_A}$ and $\widehat{q_B}$ estimators and define
\[
\widehat{p_A} = \frac{\widehat{q_A}}{\widehat{q_A}+\widehat{q_B}}
\]
and similarly 
\[
\widehat{p_B} = \frac{\widehat{q_B}}{\widehat{q_A}+\widehat{q_B}}
\]
as our final estimators of $p_A$ and $p_B$ respectively. Since $\widehat{q_A}$ and $\widehat{q_B}$ must be positive then $\widehat{p_A}$ and $\widehat{p_B}$ will be positive and sum to one. 

In this section we have discussed a simplified version of the cell type deconvolution problem. Next we will discuss in more detail the linear modeling of the oligo concentrations and expressions that is at the heart of our method. We will then develop a more comprehensive deconvolution method that can incorporate multiple marker oligos and multiple cell types. In closing we will highlight how this model differs from those existing in the literature.

\section{Concentrations and Expressions}

Let's look concretely at the relationship between the concentration of oligos in our cell samples and the expression measurements the DNA microarray produces. To do this we will look at the Affymetrix Latin Square data set (\cite{latin}). This data set consists of 14 microarray experiments (with three technical replicates each for a total of 42 experiments) where known concentrations of 42 different transcripts were spiked into a hybridization solution containing a complex human RNA background mix. These transcripts were spiked in at known concentrations ranging from 0.125pM to 512pM. This data set is very useful because for a handful of oligos we know the concentration and can look correspondingly at their measured expressions.

Let's work with un-summarized probe level data at the $\log$ level. That is, in this case the oligos $n=1,\ldots,N$ are the individual probes on the microarrray. The microarray experiments reported in this data set were done using a U133A Affymetrix GeneChip and thus measure $N=506,944$ probes per experiment. For each of these probes the microarray data consists of an intensity measurement $I_n$. We are going to work with $\log_2$ level data doing analysis with the $\log_2$ probe measurements $Y_n \overset{def}{=} \log_2(I_n)$. In Figure \ref{fig:c1} we have plotted the $\log$ intensity measurement, $Y_n$, against the corresponding log concentration for oligo $n=479376$. There is nothing special about this oligo it simply exemplifies the relationship between expression and concentration. 

\begin{figure}[ht]
  \centering
<<c1,eval=TRUE,cache=TRUE,echo=FALSE,warning=FALSE,fig.height=5,fig.width=5>>=
@
\caption{Expression of probe 479376 against concentration on a $\log-\log$ level.}
  \label{fig:c1}
\end{figure}

We can see from looking at this figure that there is a positive relationship (on the $\log$-$\log$ level) between the concentration and expression level of the probe. Notably, however, the relationship is not linear. Indeed the curve tends to flatten out for very high and very low concentrations. One might imagine that the true relationship on this scale is something of a sigmoid curve. We fit such a curve to our data in Figure \ref{fig:c2}.

\begin{figure}[ht]
  \centering
<<c2,eval=TRUE,cache=TRUE,echo=FALSE,warning=FALSE,fig.height=5,fig.width=5>>=
@
\caption{Expression of probe 479376 against concentration on a $\log-\log$ level. Sigmoid curve fit to points.}
  \label{fig:c2}
\end{figure}

This simple sigmoid curve fits the expression-concentration data very well on the $\log$-$\log$ level. The important features of this graph are the flattened tail below about zero, the flattened tail above about eight and, since the sigmoid curve is smooth, the approximately linear relationship between the two tails. In retrospect, this sort of curve isn't too surprising. Indeed some sort of sigmoid curve seems likely given the constraints of the fluorescently activated probes. The probes have a finite maximum amount they can fluorescently shine and hence a saturation point beyond which increasing concentration does not increase expression measurement. This is the right-hand plateau after the $\log_2$ concentration rises above about $8$. On the lower end there is a flat tail on the left-hand side. This is likely because there is some background fluorescence off the microarray probes even when none of the corresponding oligos are present. This might be caused by phenomena like competitive cross hybridization. Thus even as the concentration of the oligo approaches zero the $\log$ intensity doesn't go to negative infinity because there is background fluorescence. Thus we see, instead, the the expression measurement plateau's for low concentrations as well as for high. Both of these phenomena mean that the expression measurement is bounded. Thus the most simple and smooth manner for which the concentration to move from the lower bound to the upper bound as we increase concentration is going to look something like a sigmoid curve. This likely explains why we see such a good fit. Obviously there is no guarantee that there be such a nice relationship between expression and concentration however given the simple continuous fluorescence process it's not too surprising, either.


While this sigmoid curve fits out data quite well we do not use a sigmoidal model in our deconvolution method. Remember that we modeled the $\log_2$ expression, $Y_n$, as linear in the $\log_2$ concentration such that 
\[
Y_n =\log_2(I_n) = \theta_n + \gamma\log_2\left(\eta_n\right) + \epsilon_n
\]
where $\eta_n$ is the concentration of oligo $n$ in the hybridization solution, $\theta_n$ is the oligo-specific intercept, $\gamma$ is the slope for all oligos and $\epsilon_n$ is the oligo-specific error term. We plot such a linear curve for our data in Figure \ref{fig:c3}.

\begin{figure}[ht]
  \centering
<<c3,eval=TRUE,cache=TRUE,echo=FALSE,warning=FALSE,fig.height=5,fig.width=5>>=
@
\caption{Expression of probe 479376 against concentration on a $\log-\log$ level. Sigmoid fit in red, linear fit in blue.}
  \label{fig:c3}
\end{figure}  

The linear fit in Figure \ref{fig:c3} captures the relationship between the expression and concentration quite well so long as the concentration is not too high nor too low. Obviously the linear fit is not quite as good as the sigmoidal fit however there are some advantages. In the figures plotted in this section we assume we know both concentration and expression. However in a real deconvolution setting we know only the expression and wish to determine the concentration. Indeed our deconvolution method involves using the posited relationship to go from expressions to concentrations. Unfortunately the sigmoidal curve is not a very well behaved curve in this direction. As we increase our expression towards the maximum of our sigmoidal curve the concentration as predicted by the sigmoidal curve will increase without bound. At such high levels of expression small changes in expression will increase the predicted concentration drastically. If instead of working with the red sigmoidal curve we look at the blue linear fit then we see that the predicted concentration does not increase very quickly without bound at such high levels. Indeed it increases linearly with a slope of $\gamma$ at all points. One might view the blue linear fit as some sort of regularized version of the sigmoidal curve which keeps the model from behaving poorly at very high expressions. In a similar sense the linear fit is regularized for low expressions. An expression of, say, five would be predicted as a $\log_2$ concentration near negative infinity from the sigmoidal fit whereas the linear fit predicts a $\log_2$ concentration around zero. Thus our linear fit seems to track the true sigmoidal relationship when the expressions are neither very high nor very small. However for edge cases the linear fit regularizes our estimator keeping the a very highly expressed oligo from dominating the procedure by estimating a very high concentration. At the extreme we see that the red sigmoidal curve has a finite maximum and minimum. Thus if we encounter an expression above or below these extrema our sigmoidal model doesn't give us a straight-forward way of recovering concentration estimates since the curve is not invertible for all real numbers. Our linear model doesn't encounter such problems. For these reasons we choose to model the relationship between concentration and expression in a linear fashion. 
  
\section{A General Model}

Now that we have discussed the manner in which we model the relationship between concentration and expressions let's develop a general model for deconvolution of several cell types by multiple marker oligos. 

Let's assume that we have $K$ cell types under consideration and microarray technology which can measure $N$ oligos. Define $\eta_{kn}$ to be the concentration of oligo $n$ in hybridization solution produced by cells of type $k$ where $k\in\llbracket 1,K\rrbracket$ and $n\in\llbracket 1,N\rrbracket$. Assume that for each cell type $k=1,\ldots,K$ we have $\nu_k$ pure samples of cells of type $k$ only. For each of these samples we generate hybridization solution and run a microarray experiment. From this we get microarray data 
\[
Z_{kr} \in \mathbb{R}^{1\times N}
\]
for $k=1,\ldots,K$ and $r=1,\ldots,\nu_k$. Let's assume that these $Z_{kr}$ are $\log_2$ transformed as previously discussed. In this case then our linear model relating the concentrations $\eta_{kn}$ and the expression measurements $Z_{krn}$ tells us that
\[
Z_{krn} = \theta_n + \gamma \log_2\left(\eta_{kn}\right)+\epsilon_{krn}
\]
where $Z_{krn}$ is the measurement of the $n^{th}$ oligo in the $r^{th}$ pure sample of type $k$ and the $\epsilon_{krn}$ are mutually indpendent across $k,r$ and $n$.

Now assume we also have some heterogeneous sample that is a mixture of the $K$ cell types. This is the sample we would like to deconvolve. Let the hybridization solution of our heterogeneous sample be a mixture of pure sample hybridization solutions with mixing proportions $p_1,\ldots,p_K$. Then if $c_n$ is the concentration of the $n^{th}$ oligo in our heterogeneous sample we have that 
\[
c_n = \sum_{k=1}^{K}p_k\eta_{kn}.
\]

We run DNA microarray analysis on the heterogeneous sample's hybridization solution and get $Y\in\mathbb{R}^{1\times N}$, the $\log_2$ level expressions of oligos $1,\ldots,N$ in the heterogeneous sample. Then our linear model tells us that $Y_n$, the $n^{th}$ oligo expression measurement from this heterogeneous sample, is related to $c_n$ as
\[
Y_n = \theta_n + \gamma\log_2\left(c_n\right)+\epsilon_n.
\]
We assume that the $\epsilon_n$ and the $\epsilon_{krn}$ from the pure samples are mutually independent with mean zero and some finite fixed variance $\sigma^2$.

In order to deconvolve the heterogeneous sample assume there are some number of marker oligos for each cell type $k\in\llbracket1, K\rrbracket$. Let $G_k\subset\llbracket 1, N\rrbracket$ be the set of marker oligos for cell type $k$ such that if $n \in G_k$ then $\eta_{tn}=0$ for all $t \neq k$. That is, if $n$ is a marker oligo of cell type $k$ then it is not expressed in any other cell type than $k$. Assume that the marker oligos are mutually exclusive across cell types such that $G_k\cap G_t=\emptyset$ for all $t \neq k$. 

Now consider some oligo $n$ that is a marker of cell type $k$ such that $n\in G_k$. Then we have that
\[
c_n = \sum_{t=1}^{K} \eta_{tn}p_t = \eta_{kn} p_k
\]
as $\eta_{tn}=0$ for all $t\neq k$.
That is, if $n$ is a marker oligo of cell type $k$ then the concentration of $n$ in the heterogeneous sample is simply the concentration of $n$ in any of the pure type-$k$ samples, $\eta_{kn}$, scaled down by the mixing proportion $p_k$.

Thus if $n\in G_k$ we have that 
\[
\begin{aligned}
  Y_n &=\theta_n + \gamma\log_2\left(c_n\right)+\epsilon_n\\
  &= \theta_n + \gamma\log_2\left(\eta_{nk}p_k\right)+\epsilon_n\\
  &= \theta_n + \gamma\log_2\left(\eta_{nk}\right)+\gamma\log_2\left(p_k\right)+\epsilon_n 
\end{aligned}
\]
and
\[
\begin{aligned}
  \overline{Z_{kn}}\overset{def}{=}\frac{1}{\nu_k}\sum_{r=1}^{\nu_k}Z_{krn} &= \frac{1}{\nu_k}\sum_{r=1}^{\nu_k}\left(\theta_n + \gamma \log_2\left(\eta_{kn}\right)+\epsilon_{krn}\right)\\
  &= \theta_n + \gamma \log_2\left(\eta_{kn}\right)+\overline{\epsilon_{kn}}
\end{aligned}
\]
where $\overline{\epsilon_{kn}} \overset{def}{=} \frac{1}{\nu_k}\sum_{r=1}^{\nu_k}\epsilon_{krn}$. Thus
\[
  Y_n - \overline{Z_{kn}} = \gamma\log_2\left(p_k\right)+\epsilon_n - \overline{\epsilon_{kn}}
\]
and so if we average such terms over all marker oligos $n\in G_k$ we get
\[
\begin{aligned}
E_k &\overset{def}{=} \frac{1}{\Gamma_k} \sum_{n\in G_k}\left(Y_n-\overline{Z_{kn}}\right)\\
&=\frac{1}{\Gamma_k} \sum_{n\in G_k} \left(\gamma\log_2\left(p_k\right)+\epsilon_n - \overline{\epsilon_{kn}}\right)\\
&=\gamma\log_2\left(p_k\right)+\frac{1}{\Gamma_k} \sum_{n\in G_k}\left(\epsilon_n - \overline{\epsilon_{kn}}\right)
\end{aligned}
\]
where $\Gamma_k \overset{def}{=} |G_k|$. If we knew $\gamma$ then we could solve for $p_k$, approximately, as
\[
\exp_2\left(\frac{E_k}{\gamma}\right) = \lambda_k p_k
\]
where $\lambda_k = \exp_2\left(\frac{1}{\Gamma_k} \sum_{n\in G_k}\left(\epsilon_n - \overline{\epsilon_{kn}}\right)\right)$ is some multiplicative error term. Since we don't know $\gamma$ let's suppose we can estimate $\gamma$ by some estimator $\widehat{\gamma}$ and then plug into the previous equation to get an estimator 
\[
\widehat{q_k} = \exp_2\left(\frac{E_k}{\widehat{\gamma}}\right)
\]
with the hopes that if $\widehat{\gamma}\rightarrow\gamma$ then $\widehat{q_k}\rightarrow \lambda_kp_k$.

Ideally these $\widehat{q_k}$ would be our final estimators of the $p_k$. However in reality there is no guarantee that the collection of these $\widehat{q_k}$ are bounded above by one and sum to one. So we opt instead to re-normalize the $\widehat{q_k}$ and define the estimators
\[
\widehat{p_k} = \frac{\widehat{q_k}}{\sum_{t=1}^{K}\widehat{q_t}}
\]
as our final estimators of the $p_k$. These $\widehat{p_k}$ are all non-negative and sum to one as desired. 

In the next section we will contrast our estimators with those discussed in the literature. Then we will dig into the details of how to estimate $\gamma$ and how to choose the marker oligos.

\section{Comparison}

Let's compare our method to the most similar methods existing in the deconvolution literature. Our method estimated the mixing proportions $p_k$ through relationships between the $p_k$, $Y$, and the $Z_{kr}$ where $Y$ and $Z_{kr}$ are $\log$-level expression measurements. Some of the methods in the literature worked at the linear level working instead with $X$ and $W_{kr}$ such that $Y_n = \log_2(X_n)$ and $Z_{krn}=\log_2(W_{krn})$ so that $X$ and the $W_{krn}$ are the linear-level expression measurements. These methods would use the microarray expressions from pure samples, the $W_{kr}$, to choose marker genes and generate pure cell type expression profiles as encoded in the profile matrix $U\in\mathbb{R}^{K\times N}$. One obvious way to create $U$ is to let the $k^{th}$ row of $U$ be the average of $W_{kr}$ from $r=1,\ldots,\nu_k$. That is,
\[
U_k = \frac{1}{\nu_k} \sum_{r=1}^{\nu_k}W_{kr}
\]
so that $U$ is the matrix of characteristic expression profiles of the $K$ types in so much as row $k$ is the typical expressions of the $N$ oligos in a pure sample of cell type $k$. 
From here the model assumes that $M=(p_1,\ldots,p_K) \in \mathbb{R}^{1\times K}$ and posits the relationship
\[
X = MU + E
\]
where $E\in\mathbb{R}^{1\times N}$ is some random matrix of errors. Thus, 
\[
X_n = \sum_{k=1}^{K}p_kU_{kn}+e_{n}
\]
so that the model posits that for each oligo $n$ the expression measurement in the heterogeneous sample, $X_n$, is a convex combination of the expression of this oligo in each of the pure samples (i.e. the characteristic profiles) $U_{kn}$ with weights as the mixing proportions $p_k$ and some error term $e_n$. If we $\log$-transform both sides then we get that the model posits
\begin{equation}
\label{eqn:complit}
Y_n = \log_2\left(X_n\right) = \log_2\left(\sum_{k=1}^{K}p_kU_{kn}+e_{n}\right).
\end{equation}
Compare this to the linear model our method posits. We model the relationships as 
\[
c_n = \sum_{k=1}^{K}p_k\eta_{kn}
\]
and 
\[
Y_n = \theta_n + \gamma\log_2\left(c_n\right)+\epsilon_n
\]
so that
\begin{equation}
\label{eqn:compours}
Y_n = \theta_n + \gamma\log_2\left(\sum_{k=1}^{K}p_k\eta_{kn}\right)+\epsilon_n.
\end{equation}

There are striking simliarities between a standard model posited in the literature in Equation \ref{eqn:complit} and our model in Equation \ref{eqn:compours}. Both equations model the $Y_n$ as the $\log$ of a convex combination with weights that are the mixing proportions $p_1,\ldots,p_K$. However there are several important differences between the methods. First is the placement of the error terms $e_n$ and $\epsilon_n$. Secondly, there are differences in the terms in the convex combination. Thirdly, our model includes slope and intercept terms $\gamma$ and $\theta_n$. Finally, although not aparent from the modeling equations alone, there are differences in fitting methods among the models. 

The differenes in the placements of the error terms is not very important. This essentially arises because one model is fit at the linear level and the other is fit on the $\log$ level. More importantly, most of the methods in literature do ont explicitly include a random error term. We add an error term to their models assuming that since they estimate the $p_k$ by by minimizing terms like $||X-MU||$ over $M$ they are implicitly assuming some sort of linear error term at the linear level. The authors put very little emphasis on such terms and hence focusing on something the authors do not explicitly state does not seem important. 

The second difference between our model and a typical model in the literature is that while our model posits $Y_n$ is dependent on the $\log$ of the convex combination $\sum_{k=1}^{K}p_k\eta_{kn}$ those models in the literature instead assume that $Y_n$ is dependent on the log of the convex combination $\sum_{k=1}^{K}p_kU_{kn}$. The difference here being that the models in the literature assume some sort of direct relationship of $Y_n$ on the characteristic \emph{expressions} $U_{kn}$ while our model posits a relationship of $Y_n$ on the \emph{concentrations} of such oligos in the pure sampes, $\eta_{kn}$. Thus our model uses concentrations while theirs use expressions.  Now obviously the characteristic expression profiles $U_{kn}$ are typical generated from such pure sample expressions $W_{krn}$ and thus themselves depend on the concentration of the oligos $\eta_{kn}$. However the oligo expressions $W_{krn}$ and the oligo concentrations $\eta_{kn}$ are not interchagable. Most methods in the literature seem to assume that they are. That is, most methods to date assume that the expression measurements from the microarrays are indistinguishable from the concentration of those oligos in the samples. The implicit assumption is that the expressions directly give us the concentration of the oligos. Obviously there is a relationship between the expressions and the concentrations. However in our method we separate the two quantites modeling the expressions as a linear function of the concentrations. We believe that this distinction is crucial to accurate modeling of the convolution phenomena. Effectively most methods in the literature assume that $W_{krn} = \eta_{krn}$ such that $Z_{krn} = \log_2\left(\eta_{kn}\right)$ while in our method we model the relationship as $Z_{krn}=\theta_n+\gamma\log_2\left(\eta_{kn}\right)+\epsilon_{krn}$ including an intercept $\theta_n$ and a slope $\gamma$ to explain the relationship. This difference in modeling can be seen again in the modeling equation of $Y_n$. In Equation \ref{eqn:compours} we include the slope and intercept when modeling the dependence of $Y_n$ on $\log_2\left(\sum_{k=1}^{K}p_k\eta_{kn}\right)$ while the prototypical method in the literature typified by Equation \ref{eqn:complit} directly set $Y_n$ equal to $\log_2\left(\sum_{k=1}^{K}p_kU_{kn}\right)$. Thus our model is more complex than those in the literature because it introduces intercept parameters $\theta_n$ and a slope parameter $\gamma$. Notice that if we set each of these $\theta_n$ to zero and $\gamma$ to be one then we get something very similar to Equation \ref{eqn:complit}.

While there are some similarities in the models used by methods in the existing literature and those used in our method there are big differences in how the two models are fit. Most of the methods in the literature view the $p_k$ as parameters in a linear model and estimate the $p_k$ according to some classic method like least squares or convex optimization. For our technique we do not view the mixing proportions as parameters in a linear model. Instead we estimate our slope parameter $\gamma$ from an independent data set and then solve for the mixing proportions directly in the model using information from the pure samples to account for the $\theta_n$ and the $\eta_{kn}$. The fitting method is much different than any of the classic methods used in the literature. Furthermore our method distinguishes itself from other algorithms by the data on which it works. Most other methods work on data that has been highly pre-processed by algorithms like RMA or MAS5. Those algorithms summarize the low-level probe data into gene-level data. While looking at gene-level data in this way can be helpful our method does not require any such preprocessing. Instead we work with very low level raw intensity measurements from the microarrays at the level of probes on the array rather than summarizations of probes into genes. 

Finally it should be noted that the conversation in this section was based upon algorithms that modeled the expressions as linear at the linear (non-$\log$) level. The models used such  linear-level algorithms are most comparable to our own. Thus our conversation has focused upon them. However fully half of the algorithms we reviewed in the literature were not such models. Indeed many methods modeled expressions as linear relationships at the $\log$ level. These types of algorithms are even more different to our own. Briefly, $\log$-level models typical model the expression as something along the lines of 
\[
Y_n = \sum_{k=1}^{K}p_k \log_2\left(U_{kn}\right) + e_{n}
\]
which is a completely different model than ours. Adding an intercept $\theta_n$ and slope $\gamma$ will not reconcile the differences between these $\log$-based models and ours since the summation and parameters $p_k$ have been brought outside the $\log$.

In conclusion then there are some methods which model expressions as linear models at the linear level. These are similar to our algorithm however they use a much simpler model than our own. There are also methods which model expressions as linear at the $\log$ level. These models are fundamentally different than the model we propose. In any case, algorithms at both the linear and $\log$ levels fit their parameters as classic linear models which is much different to our estimation procedure. 


\section{Estimating $\gamma$ and Choosing Markers.}

The estimators $\widehat{p_k}$ depend on being able to estimate $\gamma$ and being able to choose marker oligos for each of the types. Here we will discuss the details of how those tasks are accomplished. 
  
\subsection{Estimating $\gamma$}
  
Now the parameter $\gamma$ is a global parameter which does not depend on which oligo measurement we are considering. Indeed if the precise pre-processing of our data $Y$ is fixed then this parameter can be estimated once for all data sets. The ability to do such an estimation depends on the existence of a data set linking the concentration (or amounts) of oligos in a data set with the expression measurements by the profiling technology. 

For technology like DNA microarrays there are many such data sets from which we can estimate $\gamma$. In particular we use the Latin Square data set (\cite{latin}). In this experiment a DNA microarray was performed on several samples in which were spiked in a common background human RNA cocktail as well as particular oligonucleotides at known concentrations. A simple linear regression model was then fit to trimmed data regressing expression levels on concentration for those measurements with known concentration. The estimated slope from this model was then used as $\widehat{\gamma}$. This method is very simple and can definitely be improved. It also requires that there be data sets with known concentrations and expressions. Nicely, however, $\gamma$ needs only be estimated once for each technology and normalization regime. 

\subsection{Choosing Markers}

The last step in describing our complete deconvolution algorithm is to describe the manner in which marker oligos are chosen. As discussed in Chapter \ref{chap:Lit} there is quite a bit of variety in how marker genes are chosen. Here we decide to leverage the DNA microarray experiments on pure samples, those $Z_{kr}$, to determine which oligos are indicative of one, and only one, type. To do this let 
$$
Z_n = ((Z_{11})_n,\ldots,(Z_{1R_1})_n,(Z_{21})_n,\ldots,(Z_{2R_2})_n,\ldots,(Z_{K1})_n,\ldots (Z_{KR_K})_n)
$$
be the measurements of the $n^{th}$ oligo across all the $Z_{kr}$. Similarly let $X_k$ be the indicator for $Z$ such that $(X_k)_i=1$ if and only if $(Z_n)_i$ is a measurement coming from a pure cell sample of type $k$. Precisely, we say that if $R_{k^-} = \sum_{i=1}^{k-1} R_i$ then $(X_K)_i = 1$ if $R_{k^-} < i \leq R_{{(k+1)}^{-}}$ and zero elsewhere. For each $n$ and $k$ we fit the simple linear regression of $Z_n$ on $X_k$ and estimate a slope $\widehat{\beta_{kn}}$. Using these slopes we assign a top type to each oligo $n$ defining sets $\tau_k$ of the top oligos for each cell type $k$ so that 
$$
\tau_k = \left\{n \left| k=\arg\max_{t=1,\ldots,K}\widehat{\beta_{tn}}\right.\right\}
$$
so that $\tau_k\subset \llbracket 1,n \rrbracket$ is the set of oligos for which cell type $k$ has the largest slope among the regressions. 
Given some parameter $M$, the number of marker oligos we wish to use, we then define the marker oligos for type $k$ as
$$
G_k = \left\{n \in \tau_k \left|\widehat{\beta_{kn}} \geq \widehat{\beta_{k(M)}}\right.\right\}
$$
where $\widehat{\beta_{k(M)}}$ is the $M^{th}$ order statistic of the set $\left\{\widehat{\beta_{kn}}\left|n\in\tau_k\right.\right\}$. This is to say that $G_k$, the set of marker oligos for cell type $k$, is comprised of the set of oligos $n$ for which the slope $\widehat{\beta_{kn}}$ is one of the top $M$ slopes among all oligos where cell type $k$ has the largest slope across all cell types. 

The slopes from our regression of $Z_n$ on the indicator $X_k$ tell us about how much the oligo $n$ differentiates cell type $k$ from all other oligos. Thus we pick those oligos to be markers if they are the best at discriminating between cell type $k$ and the other types. Implicit in having a large regression slope is that the oligo $n$ must be much more expressed in cell type $k$ than other types. Thus satisfying the definition of a marker oligo. There are definitely simpler ways of choosing marker oligos which give good results. However our hope is to extend and robustify this marker oligo selection using methods like RUV \citet{Gagnon-Bartsch2012}. In this light, then, the regression framework is useful. Choosing the marker oligos is a very important step in any deconvolution algorithm thus much of our future work will focus on exploring precisely how to do this well. This includes determining how many marker oligos, $M$, to pick for each cell type. 
