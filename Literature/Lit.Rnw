The first computational method for cell type deconvolution is generally attributed to \cite{Venet2001}. Since then there has been much activity in this area with dozens of methods published as recently as this past year \citet{Newman2015}. Notably, the model used for cell type deconvolution is largely the same across methods. Authors use some version of a linear model. There are, however, important differences in data assumptions and fitting methods to be considered. Our attempt here is to lay out a comprehensive, yet concise, survey of existing cell type deconvolution methodology.

\section{Model Basics}

Let's assume that for any cell sample we make $N$ expression measurements. We are purposely ambiguous here about exactly what we mean by `` expression measurements.'' For a DNA microarray these $N$ measurements may be probe-level intensity measurements. They may also be gene-level expressions summarizing the probe intensities by an algorithm like MAS5 \citet{Hubbell2002} or RMA \citet{Irizarry2003}. Furthermore, while we focus on DNA microarrays, the methods presented are likely easily applicable to some transformed mRNA counts from RNA-seq. In any case each of the $N$ measurements captures the quantity of some nucleic acid in our sample. For ease of terminology we will henceforth refer to the nucleic acid fragments measured by each of the $N$ expressions as ``oligonucleotides'' or, concisely, ``oligos.'' Depending on context the measurement of a ``oligo'' may refer to the expression measurement of some mRNA fragment itself or to the summary measurement of all the mRNA transcripts from a gene. Without loss of generality we can label these oligos from $1$ to $N$. We will need to assume that some of these oligo measurements are ``characteristic'' of a particular cell type. We call these ``characteristic oligos.'' An oligo is characteristic of a particular cell type if it is abundant in cells of that type and not abundant in cells of any other type. The hope is that these characteristic oligos will allow us to distinguish among cell types. 

Usually experiments are done in batches. Let's assume that we have $S$ cell samples and conduct the same assay on each of the samples. Let $X_{sn}$ be the expression measurement of the $n^{th}$ oligo in the $s^{th}$ sample. Then the matrix $X\in \mathbb{R}^{S \times N}$ has rows $X_{s}$ each containing the expression measurement of all $N$ oligos in sample $s$. The columns of $X$, denoted $X^n$, are the expression measurements of the $n^{th}$ oligo across all samples. Furthermore posit that each of the $s$ samples is a heterogeneous mixture of $K$ cell types. Define the mixing matrix $M \in \mathbb{R}^{S \times K}$ to be so that $M_{sk}$ is the percent of sample $s$ comprised of cells of type $k$. The rows of $M$ are the mixing proportions of the $K$ cell types in the $s^{th}$ sample meaning that $M$ is row-wise a probability matrix. Finally define the matrix $U \in \mathbb{R}^{K \times N}$ to be the matrix of characteristic oligo measurements. Specifically $U_{kn}$ is the typical oligo expression of oligo $n$ in cell type $k$. Each row of the matrix, $U_{k}\in\mathbb{R}^{N}$, is what we call a oligo expression profile for type $k$. It is a vector that contains the ``characteristic'' or ``typical'' expression measurements of cells of type $k$ for each of the $N$ oligos.

By and far the most common manner to model cell-type convolution is as a linear model such that
\begin{equation}
  \label{mainmodeqn}
X = MU + E
\end{equation}
where $E$ is a random matrix of errors. Almost universally cell-type convolution is modeled as such a matrix product of $M$ and $U$ however most of the deconvolution literature does not explicitly state such a statistical model. Mainly authors do not discuss an error term $E$ much less its distribution. Thus we are correspondingly vague about $E$. 

This model is attractive because it is simple. The model posits that the convoluted expressions from heterogeneous mixtures of $K$ cell types, as captured in $X$, are linear combinations of some ``characteristic'' expressions of each of the $K$ types, captured by $U$, with weights that are simply the mixing proportions of the types in each of the samples, the matrix $M$. Put another way, cell type convolution is a linear mixing process. Some version of this linear mixing model is used in all the methods we surveyed. While the true relation between the characteristic expression profiles $U$ and the convoluted data $X$ is known to be non-linear this model has proven both approximately true and empirically quite useful \citet{Shen-Orr2010}.

Beyond this basic model there are several facets which set apart existing deconvolution techniques. Firstly there are the assumptions about which data is known. As $X$ is the data we want to deconvolve, it is always assumed that $X$ is known. However we can posit either knowing $M$ to predict $U$ or knowing $U$ to predict $M$ or knowing neither $M$ nor $U$ and jointly estimating them. Hand in hand with these data assumptions are the preprocessing normalizations and transformations applied to the data by the authors. The second major facet to a deconvolution algorithm is the marker oligos. Marker oligos are expression measurements that are particularly indicative of one cell type over the others. It has been noted almost universally that restricting analysis to these marker oligos, in one way or another, can improve model fit. Thus the manner in which the markers are chosen and applied set apart the deconvolution techniques. Finally, the approach to fitting the linear model distinguishes the methods. Part of the choice when fitting the model is what constraints to enforce. For example, we would like the mixing proportions to be positive and sum to one.

For this paper we are mostly interested in methods that can predict the mixing proportions $M$. Thus we begin by focusing on methods where $U$ is assumed to be known and $M$ is inferred. We will then talk about methods of estimating $U$ and $M$ jointly without prior knowledge or either. We will not discuss methods that assume the mixing proportions $M$ are known and attempt to predict $U$.  

\section{Known Characteristic Profiles}
\label{litrev:knownu}

Let's look at methods of cell type deconvolution where the expression profile matrix $U$ is known and one is interested in estimating the mixing matrix $M$. The simplest such algorithms comes from \cite{Abbas2009}. Here the authors work with linearly summarized gene level microarray data normalized by the  MAS5 algorithm. The cell type expression profiles are gathered either from external databases or created from expression profiles of known pure mixtures. The convolution problem is modeled as in equation \ref{mainmodeqn}. The mixing matrix $M$ is thus estimated by regressing the convoluted profiles $X$ on the known expression profiles $U$ using by a linear least squares optimization. The fit is done using only a subset of marker oligos (genes in this case) chosen by a combination of biological knowledge and differential expression of genes between the cell types as measured by absolute differences and t-test $p$-values of the gene expressions. The precise marker genes are then chosen by minimizing the condition number of the resulting sub-matrix of $U$. To enforce the sum-to-one (STO) and non-negativity (NN) constraints on the rows of the resulting estimate of $M$ a heuristic iterative algorithm is used which involves removing the smallest negative estimated regression coefficient and refitting the model. The rows of the estimated $M$ matrix are then re-scaled such that they sum to one. An implementation of their algorithm in \verb+R+ \citet{R2016} is available as part of the \verb+CellMix+ package \citet{Gaujoux2013}.

A similar approach to \citeauthor{Abbas2009} is taken by \cite{Gong2011} following the model in equation \ref{mainmodeqn}. However, in this paper \citeauthor{Gong2011} work with RMA summarized log-level gene expression data from microarrays as opposed to linearly summarized data. Marker genes are chosen in a similar fashion to \citeauthor{Abbas2009} but the model is fit using quadratic programming instead of regression. The quadratic programming framework allows explicit encoding of the NN and STO constraints. An implementation of this algorithm is also available as part of the \verb+CellMix+ package.

In a similar fashion the linear model in equation \ref{mainmodeqn} is used in \cite{Lu2003} and \cite{Wang2006} to estimate the mixing proportions $M$ using simulated annealing. \citeauthor{Lu2003} deconvolve log-level standardized microarray data using their algorithm \verb+DECONVOLUTE+ (no implementation currently available). Notably they do not use marker genes although they do mention that such genes may be useful for fitting. Meanwhile \cite{Wang2006} implement a similar algorithm at the linear gene level (normalized by MAS5) and choose to fit only using marker genes. Marker genes are found by differential expression analysis between pure cell type expression profiles using direct comparison, t-test $p$-values and step-wise discriminant analysis. No implementation of their algorithm is currently available.

Other ways to fit the linear model can be found in \cite{Qiao2012} who implement an algorithm using non-negative least squares. \cite{Qiao2012} work with log-level gene expression data normalized by RMA. The use of non-negative least squares allows them to deal with the NN constraint. The STO constraint is handled by post hoc re-normalizing the rows of the estimated $M$ matrix such that they sum to one. Marker genes are chosen by differential expression comparisons between pure expression profiles of the types. An implementation of their algorithm in \verb+Octave+ can be found in the supplemental material of their paper \citet{Qiao2012}. 

\cite{Newman2015} and \cite{Altboum2014} fit the linear model using penalized fitting methods. \citeauthor{Altboum2014} use an elastic-net penalization to deconvolve log-transformed RNA-seq data. They compare their method across several ways of choosing marker genes. Their algorithm, named \verb+DCQ+ for ``digital cell quantification'',  is available as part of the \verb+ComICS+ package in \verb+R+ \citet{ComICS2016}. On the other hand, \citeauthor{Newman2015} solve the model using support vector regression, specifically $\nu$-SVR with a linear kernel. They look at linear normalized gene-level data and, similar to others, choose marker genes looking at differential expressions across groups by $p$-values and the 2-norm condition number of the resulting cell type profile sub-matrices. The NN constraint is dealt with by zeroing out negatively estimated mixing proportions and the STO constraint is then satisfied by re-scaling the mixing proportions so they sum to one. Their algorithm, dubbed \verb+CIBERSORT+, is available in \verb+R+ from their proprietary website \citet{Newman2015}.

Finally there has been some work on Bayesian modeling of deconvolution. While these models are Bayesian they still respect the linear model we posit in equation \ref{mainmodeqn}. However, being Bayesian, they put priors on quantities we consider fixed, for example, the mixing proportions. The first work in this area was pioneered by \cite{Quon2009} with their algorithm \verb+ISOLATE+. The model is basically the same as that of Latent Dirichlet Allocation \citet{Blei2003} but applied to cell type deconvolution. The model is fit by  maximum posterior likelihood estimates obtained by the expectation maximization algorithm. The algorithm has since been refined and expanded but the application of an LDA-like Bayesian algorithm still is at the heart of the work (\citeauthor{Qiao2012} \citeyear{Qiao2012}, \citeauthor{Quon2013} \citeyear{Quon2013}). The latest version of the algorithm, \verb+ISOpure+ available in a \verb+Matlab+ in a supplement to their paper \citet{Quon2013}, deconvolves RMA-normalized log-level gene expression data from microarrays. A unique feature of these Bayesian algorithms is that they work without constraining the analysis to marker genes. Furthermore the NN and STO constraints are taken care implicitly by the estimates coming from the maximum likelihood Bayesian formulation.

There are a couple of points to highlight in closing. Firstly we can see that, in one way or another, all the proposed methods of estimating $M$ from $U$ use the model formulation in equation \ref{mainmodeqn}. Furthermore in basically all methods, excepting the Bayesian formulation, marker genes are used explicitly in the analysis. However these marker genes are often chosen in an ad hoc manner for each data set considered. The various ways of fitting the linear model are as follows:
\begin{enumerate*}[label=(\arabic*)]
\item least squares
\item quadratic programming
\item simulated annealing
\item non-negative least squares
\item regression with elastic-net penalization
\item support-vector regression, or 
\item maximum a posteriori estimation.
\end{enumerate*}
Some of these fitting methods deal with the non-negativity and sum-to-one constraints elegantly and other not so elegantly. Furthermore there is little agreement on how to properly normalize the data. Some methods use well-known algorithms like RMA or MAS5. Others use simpler procedures like mean-centering and standardization. There is generally no agreement on what transformations to apply to the data, for example, whether to work with data on the log level or the linear level.

Finally, as a practical point, at the time of the writing of this paper there are limited implementations currently available for these methods. \citeauthor{Gaujoux2013} have implemented the methods of \citeauthor{Abbas2009} and \citeauthor{Gong2011} in their \verb+R+ package \href{http://web.cbio.uct.ac.za/~renaud/CRAN/web/CellMix/}{CellMix}, \citeauthor{Altboum2014} have an implementation of their elastic-net based algorithm as part of the \verb+R+ package \href{https://cran.r-project.org/web/packages/ComICS/index.html}{ComICS}, \citeauthor{Qiao2012} have stand-alone implementations of their Bayesian methods in \verb+Octave+ (\href{https://github.com/gquon/PERT}{PERT}) and an \verb+R+ package (\href{https://cran.r-project.org/web/packages/ISOpureR/index.html}{ISOpure}) and \citeauthor{Newman2015} has \verb+R+ code available for \href{https://cibersort.stanford.edu/}{CIBERSORT}.

\section{Unknown Profiles and Proportions}
\label{litrev:full}

The next set of deconvolution algorithms we'd like to talk about are methods of ``full deconvolution'' \citet{Gaujoux2013}. That is, methods which attempt to predict, simultaneously, both the mixing proportions constituting the matrix $M$ and the characteristic expression profiles defining the matrix $U$. These are distinct from what \citeauthor{Gaujoux2013} call ``partial deconvolution'' methods which use either knowledge of the matrix $U$, as in Section \ref{litrev:knownu}, to predict $M$ or knowledge of $M$ to predict $U$. While full deconvolution problem uses less information than the partial methods the differences are not as straightforward as it may appear. 

One can imagine all deconvolution algorithms on a sliding scale of supervision. On one end of the scale there are highly supervised deconvolution algorithms like that in Section \ref{litrev:knownu}. There we give the deconvolution methods prototypical examples of the different cells types as encoded in the expression profile matrix $U$. This matrix $U$ is in some sense the training data for the algorithm. We may also give the algorithms supervisory information in the form of marker genes for each of the cell types. The ``full deconvolution'' methods we will discuss in this section are on the other end of this supervision spectrum. For these algorithms we do not give detailed supervision in the form of the prototypical expression profiles from the matrix $U$. Indeed there are such deconvolution algorithms which require neither cell type expression profiles nor marker genes for the cell types. These types of algorithms are truly unsupervised. Unfortunately they are also very under-constrained. Hence, such blind deconvolution is not as useful as one might hope. Indeed even with these algorithms practitioners find the need to \emph{post hoc} look at prototypical expressions for the sake of interpretability thus undercutting the attempt to avoid explicit use of known expression profiles for the cell types. Likewise, authors for most full deconvolution methods find that some level of supervision is needed for the deconvolution algorithms. Thus while the methods mentioned in this section do not make explicit use of characteristic expression profiles they do, mostly, use marker genes. However, this brings up somewhat of a contradiction as such marker gene lists do not appear out of thin air. Indeed most of the cell type marker lists available in the biological literature are created from such pure cell type expression profiles that the full deconvolution algorithms eschew. This is all to say that one should be aware that the claim that these methods operate without knowledge of $U$ is not quite as straight-forward as it may seem. These algorithms use marker genes which are, arguably, some processed form of $U$. To be fair, however, this level of supervision is less than that of the methods requiring entire cell type expression profiles as discussed in Section \ref{litrev:knownu}. Furthermore such full deconvolution methods have proven to be very accurate and are some of the most competitive methods for accurate estimation of the mixing matrix $M$. This is all the more impressive as they use less information (i.e. supervision) than those partial deconvolution methods mentioned heretofore. Furthermore, estimating the profiles, i.e. the matrix $U$, is a fundamentally important task because it allows practitioners to use such deconvolution techniques to conduct differential expression analysis of expression profiles estimated from separate samples while accounting for the confounding effects of the mixing proportions. 

The earliest full deconvolution algorithm, and indeed the earliest computational deconvolution algorithm, is that of \cite{Venet2001}. \citeauthor{Venet2001}, like all others, use the basic linear model described in equation \ref{mainmodeqn}. The authors work with gene-level summarized microarray data which has been normalized in an ad hoc manner. Notably their algorithm is completely unsupervised using neither gene expression profiles nor marker genes. Instead they posit the non-trivial hypothesis that the number of cell types, $K$, is known. From here the the matrices $U$ and $M$ are fit by matrix factorization of $X$. The authors consider two methods of estimation. The first is an sequential two-step algorithm which attempts to minimize the objective $||X-UM||_2^2$. After starting with initial random estimates of $U$ and $M$ it bounces back and forth estimating $U$ and $M$ sequentially using a non-negative least squares optimization algorithm to estimate $U$ given the current estimate of $M$ and then $M$ given the current estimate of $U$. The non-negative least squares algorithm allows the algorithm to easily satisfy the NN constraint however at each step they must re-normalize the rows of $M$ such that they sum to one. The second method of matrix factorization that is considered is using principal components analysis or some form of factor analysis to factor $X$ as, approximately, $MU$. Unfortunately such methods do not respect the STO or NN constraints and furthermore impose orthogonality constraints which are biologically not plausible. Finally, as mentioned previously, since this method is completely unsupervised cell types are not implicitly associated with the estimated expression profiles of $U$. Much like principal components analysis there is a problem of interpretability of the columns of $U$. The authors suggest that one looks at the estimated profiles, in $U$, and associate them with cell types by comparing the estimated profiles to known expression profiles of known cell types. A mapping between estimated profiles and cell types can then be made by determining approximate matches. However such comparisons are not very practical as having expression profiles of known cell types rather undercuts the idea of doing a completely unsupervised cell type deconvolution. No software implementation of this algorithm could be found.

Following the spirit of \citeauthor{Venet2001} others have attempted to solve the full deconvolution problem with non-negative matrix factorization. Notably \cite{Repsilber2010} take a very similar deconvolution approach. They too implement a completely unsupervised algorithm using iterated sequential non-negative least squares fitting. They enforce the STO constraint similarly to \citeauthor{Venet2001}. Their method is largely an updated version of that of \citeauthor{Venet2001}. \citeauthor{Repsilber2010} work on gene-level microarray data that has been normalized by limma \citet{Ritchie2015} and RMA  but is considered at the linear level rather than the log level. Their algorithm is implemented in the \verb+R+ package \verb+deconf+ \citet{Repsilber2010} or as part of the \verb+CellMix+ package \citet{Gaujoux2013}. However, being a completely unsupervised algorithm \verb+deconf+ suffers from the same cell-type identification problems as discussed previously. \citeauthor{Repsilber2010} suggest a similar approach where one \emph{a posteriori} identifies the estimated expression profiles with cell types by comparing the estimated profiles with the known profiles of known cell types. Not only is this undesirable but, as pointed out by \cite{Gaujoux2012}, inclusion of marker gene information actually improves the estimates of $U$ and $M$.

A comprehensive comparison of non-negative matrix factorization full deconvolution methods is done in \cite{Gaujoux2012}. The authors with with linearly summarized (MAS5) gene expression data from microarrays. However these authors make use of marker genes to enforce constraints on the $U$ matrix during fitting so that that marker genes are expressed in only one cell type. The matrix factorization is done with three types of non-negative matrix factorization. The first is due to \cite{lee2000} which minimizes the Euclidean distance using gradient descent, the second is from \cite{Brunet2004} which minimizes the Kullback-Leibler divergence and the final from \cite{Pascual-montano2006} uses a constant smoothing matrix to obtain sparse results. The NN constraint is taken care of by the non-negative matrix factorization algorithms and the STO constraint is achieved by re-normalizing the rows of $M$ post hoc. The method for choosing the marker genes follows \cite{Abbas2009}. An implementation of their algorithm is available as part of the \verb+CellMix+ package as Gaujoux is the creator and maintainer of that package.

A different full deconvolution approach using marker genes is explored in \cite{Zhong2013}. Here the authors follow a two-step algorithm to estimate $M$ and $U$ from $X$. First they estimate cell type frequencies defining the matrix $M$. This is done by restricting the model to only include the marker genes instead of all genes. From here constraints on the rows of $M$ to sum to one is exploited to form a system of equations involving the unknown average marker gene expression for each cell type. This system is solved to generate the average marker gene expressions which is then used to estimate the mixing proportions for each cell type, i.e. the matrix $M$. From this matrix $M$ we can estimate the complete gene expression for each cell type by non-linear least squares solving $\arg\min_U||X-MU||_2^2$ through quadratic programming. The algorithm is used on gene-level microarray data summarized by RMA. However the authors suggest that one works at the linear scale as opposed to the log-scale. The gene used as markers are chosen from biological databases specific to the cell types in question or through differential expression analysis of known pure samples of the types. The algorithm, dubbed \verb+DSA+ for ``digital sorting algorithm'', is available in the \verb+DSA+ package in \verb+R+  \citet{Zhong2013}.

The final method which we will cite is that from \cite{Liebner2014}. The algorithm put forth by \citeauthor{Liebner2014} works with log-level gene-summarized microarray data pre-processed by RMA. If biological knowledge or pure sample expression profiles are available then their method will use this information to curate a set of markers for the various cell types. However the algorithm can work completely unsupervised if desired. In this case the genes with the top 1\% of variability in the sample data $X$ are assumed most likely to be differentially expressed across groups and are assigned putatively to a group by $k$-means clustering. This hypothesis is not necessarily straightforwardly true. Nonetheless, once marker genes are determined the model is fit on the subset of marker genes by a least squares minimization. The method is called ''microarray microdissection with analysis of differences'' or \verb+MMAD+ and is available as \verb+MATLAB+ code \citet{Liebner2014}.

The concluding remarks for the full deconvolution methods mirror those of the partial methods mentioned in section \ref{litrev:knownu} and the comments at the beginning of the section. All of the methods surveyed here use some version of a linear model for solving the full deconvolution problem. While some methods attempt to be completely unsupervised non truly achieve this end. Marker genes obtained either through biological databases or through some pure expression profiles are needed. The marker genes are either used post hoc to map estimated profiles and proportions to cell types or the markers are used to aid in the fitting process of the algorithms. Selection of marker genes seems no more sophisticated than those methods previously presented. Furthermore all fitting methods seem to be some version of non-negative matrix factorization either standard or ad hoc. There is little agreement on the correct transformations or normalizations to use as preprocessing steps. However all methods surveyed here use gene-level microarray data. Helpfully, these full deconvolution algorithms have much more supplementary code and information available. Indeed there are implementations of the algorithms of \citeauthor{Gaujoux2012}, and \citeauthor{Repsilber2010} in \verb+CellMix+, an \verb+R+ implementation of \href{https://github.com/zhandong/DSA}{DSA} by \citeauthor{Zhong2013} and a \verb+Matlab+ implementation of \href{https://sourceforge.net/projects/mmad/}{MMAD} by \citeauthor{Liebner2014}.
